# -*- coding: utf-8 -*-
"""Movement_Detection_1.0

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wBtu7FqMbsHiM8fRTvSQ7Kc5VySNJ2ia

# Import Libraries
"""

base_path = '/content/drive/MyDrive/Movement Detection Thesis/data/'

combined_path = base_path + 'combined'

extracted_path = base_path + 'extracted'

# Commented out IPython magic to ensure Python compatibility.
import os
import glob
import bz2
import pickle

import numpy as np
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

from numpy.lib.stride_tricks import sliding_window_view
from scipy.stats import mode

from sklearn.preprocessing import LabelEncoder

# %matplotlib inline

"""# Useful functions"""

def read_pbz2(path):
    with bz2.BZ2File(path, 'rb') as f:
        data = f.read()
    decomp_data = pickle.loads(data)
    return decomp_data

def get_empty_csv(directory):
    empty_files = []
    csv_files = glob.glob(os.path.join(directory, '*.csv'))
    for file_name in csv_files:
        try:
            pd.read_csv(file_name)
        except:
            empty_files.append(file_name)
    return empty_files

def get_files_dict(directory):
    pbz2_files = glob.glob(os.path.join(directory, '*.pbz2'))
    csv_files = glob.glob(os.path.join(directory, '*.csv'))
    empty_files = get_empty_csv(directory)
    file_dict = {}
    for file_name in pbz2_files:
        file_name_short = os.path.basename(file_name)
        key = os.path.splitext(file_name_short)[0]
        file_dict[key] = [file_name]

    for file_name in csv_files:
        if file_name not in empty_files:
            file_name_short = os.path.basename(file_name)
            key = file_name_short.split('_')[0]
            if key in file_dict:
                file_dict[key].append(file_name)
            else:
                file_dict[key] = [file_name]
    return file_dict

def merge_dfs(df_1, df_2):
    ts = float(df_2.Comments[0].split(' ')[1])
    df_2['t'] = df_2.Time.apply(lambda x: x + ts)
    df_2 = df_2.drop(['Time', 'Comments', 'Handedness'], axis=1)
    end = df_2.t.max()
    df_merged = pd.merge_ordered(df_1, df_2, on='t', fill_method='ffill')
    df_merged = df_merged[df_merged.t <= end]
    df_merged['Movement'] = df_merged.loc[:, 'Movement'].replace(np.nan, 'no_movement')
    df_merged['Movement'] = df_merged.loc[:, 'Movement'].replace('Glas_grabbing', 'Glass_grabbing')
    df_merged.dropna(axis=0, how='any', inplace=True)
    df_merged.set_index('t', inplace=True)

    return df_merged

"""# Data Preprocessing

## Merge Data
"""

file_dict = get_files_dict(base_path)
print('5c91085923feb407ddeb5e1c', file_dict['5c91085923feb407ddeb5e1c'])

sorted_filenames_pickles = sorted([file_name for file_name in os.listdir(base_path) if file_name.endswith('.pbz2')])

sorted_filenames_csv = sorted([file_name for file_name in os.listdir(base_path) if file_name.endswith('movements.csv')])

list_pickles = []
for file_name in sorted_filenames_pickles:
  #print(file_name)
  list_pickles.append(file_name.removesuffix('.pbz2'))

print(list_pickles)

list_csv = []
for file_name in sorted_filenames_csv:
  #print(file_name)
  list_csv.append(file_name)
print(list_csv)

def group_files_by_prefix(list1, list2):
  grouped_files = {}
  dict2 = {filename[:24]: [] for filename in list2}
  for filename in list2:
      dict2[filename[:24]].append(filename)

  for filename1 in list1:
      prefix = filename1[:24]
      if prefix in dict2:
          if prefix not in grouped_files:
              grouped_files[prefix] = []
          grouped_files[prefix].append((filename1, dict2[prefix]))

  return grouped_files

grouped_files = group_files_by_prefix(list_pickles, list_csv)
print(grouped_files)

file_1 = base_path + '5c91085923feb407ddeb5e1c.pbz2'
file_1_csv = base_path + '5c91085923feb407ddeb5e1c_0_KH2018_movements.csv'
df_1 = read_pbz2(file_1)[1]
df_2 = pd.read_csv(file_1_csv)
df_exp = merge_dfs(df_1, df_2)
df_exp.drop(columns=['att_w', 'att_x', 'att_y', 'att_z'], inplace=True)
first_column_name = df_exp.columns[0]
print(first_column_name)
df_exp.shape

file_1 = base_path + 'extracted/' + '5c9108fc23feb407ddeb5e1e.csv'
file_1_csv = base_path + '5c9108fc23feb407ddeb5e1e_0_KH2018_movements.csv'
df_1 = pd.read_csv(file_1)
df_2 = pd.read_csv(file_1_csv)
df_exp = merge_dfs(df_1, df_2)
df_exp.drop(columns=['att_w', 'att_x', 'att_y', 'att_z'], inplace=True)
#df_hopscotch = df_exp[df_exp['Movement'] == 'no_movement']
#out_file_name = base_path + 'combined/' + 'no_movement1.csv'
#df_hopscotch.to_csv(out_file_name, index=False)
#df_exp.shape
df_exp.head(100)
df_exp['Movement'].unique()

file_1 = base_path + 'extracted/' + '5c9108fc23feb407ddeb5e1e.csv'
file_1_csv = base_path + '5c9108fc23feb407ddeb5e1e_0_KH2018_movements.csv'
df_1 = pd.read_csv(file_1)
df_2 = pd.read_csv(file_1_csv)
df_exp = merge_dfs(df_1, df_2)
df_exp.drop(columns=['att_w', 'att_x', 'att_y', 'att_z'], inplace=True)
#df_hopscotch = df_exp[df_exp['Movement'] == 'no_movement']
#out_file_name = base_path + 'combined/' + 'no_movement1.csv'
#df_hopscotch.to_csv(out_file_name, index=False)
#df_exp.shape
df_exp.head(100)
df_exp['Movement'].unique()

"""For 5c91085923feb407ddeb5e1c"""

list_pickles = [
     '5c91085923feb407ddeb5e1c',
     '5c91089323feb407ddeb5e1d',
     '5c9108fc23feb407ddeb5e1e',
     '5ca4b32b23feb407ddeb7b95',
     '5ca4b35c23feb407ddeb7b96',
     '5ca4b3cb23feb407ddeb7b97',
     '5ca4b45123feb407ddeb7b99',
     '5ca4b9a123feb407ddeb7bfe',
     '5ca4b9bc23feb407ddeb7bff',
     '5ca4b9f523feb407ddeb7c00',
     '5ca4ba2823feb407ddeb7c01',
     '5ca4ba4923feb407ddeb7c02',
     '5ca4ba5123feb407ddeb7c03',
     '5ca4ba5f23feb407ddeb7c04',
     '5cadf2a223feb407ddebbd15',
     '5cadf2d223feb407ddebbd16',
     '5cadf2f523feb407ddebbd2c',
     '5cadf32723feb407ddebbd96',
     '5cadf61c23feb407ddebbe48',
     '5cadf63523feb407ddebbe49',
     '5cadf65523feb407ddebbe4a',
     '5cadf68623feb407ddebbe4b',
     '5cadf6c023feb407ddebbe4c',
     '5cadf72d23feb407ddebbe4d',
     '5cadf72d23feb407ddebbe4e',
     '5cadf7eb23feb407ddebbe4f',
     '5cadf85123feb407ddebbe50',
     '5ccbfe2523feb407ddebce00',
     '5ccc0c3523feb407ddebceb1',
     '5ccc0cab23feb407ddebceb2',
     '5ccc21d623feb407ddebcedc',
     '5ccc24be23feb407ddebcf07',
     '5cd4553223feb407ddebfb60',
     '5cd455e623feb407ddebfb62',
     '5cd4581223feb407ddebfb64',
     '5cd4584d23feb407ddebfb65',
     '5cd9413b348ecf5254faa72e',
     '5cd946b0348ecf5254faa871',
     '5db9a5e585c2e10b4641ec4f',
     '5dc59072544039096f98348e',
     '5dc590d7544039096f98348f',
     '5dc590fd544039096f983490',
     '5dcc2a138a03930ee46d34f7',
     '5defc093830c310a053c0479',
     '5defc0eb830c310a053c047a',
     '5dfa36a05528491864674e0c',
     '5e208ab3ac51621e2c0d4333',
     '5e21da55ac51621e2c0d4334',
     '5e26f98db3d7be09a6d998ea',
     '5e2eefe5990ae67f6f4044fb',
     '5e38034e990ae67f6f40536a',
     '5e4d55af990ae67f6f406a16',
     '5e4fa5e3990ae67f6f408561',
     '5e4fad30990ae67f6f408562',
     '5e4fad9c990ae67f6f408577',
     '5e54faab308d5912ce50d68c',
     '5e5d394e3a210055f9c12c87',
     '5e5d3a4c3a210055f9c12c88',
     '5e6a72353a210055f9c12f45',
     '5f17018f7286ff09659275f5',
     '5f2286b67286ff0965928f13',
     '5f293e627286ff096592d63e',
     '5f4665b7fcd1180b61558a93',
     '5f74b018fcd1180b6155a984',
     '5f7c8863fcd1180b6155a9c7',
     '5f85cf3ffcd1180b6155a9ca',
     '5faa65669369d40a2a438f37',
     '5faa6a2e9369d40a2a438f39',
     '5fb7f33e864b390a1306c3b6',
     '5fd77d2beeb5681070391899',
     '60183750dd1d0b0a43331e58',
     '60183793dd1d0b0a43331e59',
     '603dfcdcdd1d0b0a433896bb',
     '604a2486dd1d0b0a43394b4a',
     '60742ba6dd1d0b0a433e6527',
     '60742bbfdd1d0b0a433e652d',
     '6081466f933150098e403485',
     '608801c4933150098e40349a',
     '60953c0d933150098e4145ed',
     '60c09e27933150098e4157e8',
     '60cc5f67933150098e417e03'
]
file_count_dict = {}
for f in list_pickles:
    filename = f
    file_1 = os.path.join(base_path, 'extracted', filename + '.csv')
    file_1_csv = os.path.join(base_path, filename + '_0_KH2018_movements.csv')

    # Check if files exist and are not empty before proceeding
    if os.path.exists(file_1) and os.stat(file_1).st_size > 0 and \
       os.path.exists(file_1_csv) and os.stat(file_1_csv).st_size > 0:

        df_1 = pd.read_csv(file_1)
        df_2 = pd.read_csv(file_1_csv)
        df_exp = merge_dfs(df_1, df_2)

        df_exp.drop(columns=['att_w', 'att_x', 'att_y', 'att_z'], inplace=True)

        combined_dir = os.path.join(base_path, 'combined_movements_for_edge_impulse')
        if not os.path.exists(combined_dir):
            os.mkdir(combined_dir)

        for movement in df_exp['Movement'].unique():
            movement_dir = os.path.join(combined_dir, movement)
            if not os.path.exists(movement_dir):
                os.mkdir(movement_dir)

            # Use the dictionary to determine the file number
            file_count = file_count_dict.get(movement, 0)
            out_file = os.path.join(movement_dir, f"{movement}_{file_count}.csv")
            df_exp[df_exp['Movement'] == movement].to_csv(out_file, index=False)

            # Update the dictionary with the new count
            file_count_dict[movement] = file_count + 1
    else:
        print(f"Skipping {filename} due to missing or empty file.")

for f in sorted(os.listdir(base_path + 'extracted/')):
  print(f)

out_file = base_path + 'combined/' + 'fox1.csv'
df_exp[df_exp['Movement'] == 'Lame_fox'].to_csv(out_file)

key_word = 'Peck'
out_file = base_path + 'combined/' + key_word + '1.csv'
df_exp[df_exp['Movement'] == key_word].to_csv(out_file)

key_word = 'Pray'
out_file = base_path + 'combined/' + key_word + '1.csv'
df_exp[df_exp['Movement'] == key_word].to_csv(out_file)

key_word = 'Nose'
out_file = base_path + 'combined/' + key_word + '1.csv'
df_exp[df_exp['Movement'] == key_word].to_csv(out_file)

key_word = 'Clapping'
out_file = base_path + 'combined/' + key_word + '1.csv'
df_exp[df_exp['Movement'] == key_word].to_csv(out_file)

out_file = base_path + 'combined/' + 'bear1.csv'
df_exp[df_exp['Movement'] == 'Bear'].to_csv(out_file)

df_no_movement2 = df_exp[df_exp['Movement'] == 'no_movement']
out_file_name_no_movement_2 = base_path + 'combined/' + 'no_movement2.csv'
df_no_movement2.to_csv(out_file_name_no_movement_2)

df_spider1 = df_exp[df_exp['Movement'] == 'Spider']
out_file_name_spider_1 = base_path + 'combined/' + 'spider1.csv'
df_spider1.to_csv(out_file_name_spider_1)

df_spider1.head(30)

df_combined = pd.read_csv(out_file_name)
df_combined.head(30)



print("Header Names:", df_exp.columns.tolist())
print("Data Types:\n", df_exp.dtypes)

import os
import pandas as pd
import glob
from collections import defaultdict

# Directory containing the files
directory = base_path

# List all files and sort them
files = sorted(os.listdir(directory))

# Group files by the base identifier
file_groups = defaultdict(list)
for file in files:
    identifier = file.split('_')[0]
    file_groups[identifier].append(file)

# Process each group of files
output_dir = directory + 'merged_files/'
if not os.path.exists(output_dir):
    os.makedirs(output_dir)

for identifier, filenames in file_groups.items():
    if len(filenames) < 2:
        continue  # Skip groups with less than two files

    # Assume the first file is the primary and others are to be merged to it
    primary_df = pd.read_csv(directory + filenames[0])
    for filename in filenames[1:]:
        secondary_df = pd.read_csv(directory + filename)
        primary_df = merge_dfs(primary_df, secondary_df)

    # Save the merged DataFrame
    primary_df.to_csv(output_dir + identifier + '_merged.csv')
    print(f'Merged {len(filenames)} files for {identifier} into {identifier}_merged.csv')

unique_movements = df_exp['Movement'].unique()
print("Unique Movements:", unique_movements)

def merge_dfs(df1, df2):
    # Assuming you want to merge on columns that exist in both DataFrames
    common_columns = set(df1.columns) & set(df2.columns)
    return pd.merge(df1, df2, on=list(common_columns), how='inner')

dataframes = {}
files = sorted(os.listdir(base_path))



files = sorted(os.listdir(base_path))
movement_label_set = {}
label_file_ending = '_0_KH2018_movements.csv'
for file in files:
  if file.endswith(label_file_ending):
    movement_label_set.add((pd.read_csv(file))['Movement'].unique)
  else:
    continue


# # Loop through each file in the directory
# for file in files:
#     full_path = os.path.join(base_path, file)
#     if file.endswith('.pbz2'):
#         df = read_pbz2(full_path)
#         identifier = file.split('.')[0]
#     elif file.endswith('_0_KH2018_movements.csv'):
#         df = pd.read_csv(full_path)
#         identifier = file.split('_0_KH2018_movements.csv')[0]

#     if identifier in dataframes:
#         # Merge existing DataFrame with the new one
#         dataframes[identifier] = merge_dfs(dataframes[identifier], df)
#     else:
#         # Start a new DataFrame entry
#         dataframes[identifier] = df

# Process the merged DataFrames
for identifier, df in dataframes.items():
    df.drop(columns=['att_w', 'att_x', 'att_y', 'att_z'], inplace=True)
    print(f"DataFrame for {identifier} for {identifier}: shape {df.shape}")

set_movements = {"Building_blocks"}

# Function to find and process the CSV file
def find_unique_movements(base_path):
    # Loop through each file in the directory
    for file in os.listdir(base_path):
        if file.endswith('.csv'):  # Check if the file is a CSV
            full_path = os.path.join(base_path, file)  # Construct the full file path
            try:
                df = pd.read_csv(full_path)  # Try to read the CSV file into a DataFrame
                if 'Movement' in df.columns:  # Check if 'Movement' column exists
                    unique_movements = df['Movement'].unique()  # Find all unique movements
                    print(f"Unique movements in {file}: {unique_movements}")
                    for u in unique_movements:
                      set_movements.add(u)
                      print('Added ', u)
                    #return unique_movements  # Return the unique movements
                else:
                    print(f"'Movement' column not found in {file}")
            except Exception as e:
                print(f"Failed to read {file}: {e}")

# Call the function with your specified base path
unique_movements = find_unique_movements(base_path)

print(len(set_movements))
set_movements

"""### All types of movement in all of the files"""

all_movements = []

for file in os.listdir(base_path):
    if file.endswith('.pbz2'):
        df = read_pbz2(os.path.join(base_path, file))
    elif file.endswith('_0_KH2018_movements.csv'):
        df = pd.read_csv(os.path.join(base_path, file))
    else:
        continue

    if 'Movement' in df.columns:
        all_movements.extend(df['Movement'].tolist())

unique_movements = set(all_movements)

print("Unique Movements from all files:", unique_movements)

plot_features = df_exp.iloc[:500, 1:-1]
plot_features.index = df_exp.index[:500]
_ = plot_features.plot(subplots=True)

df_exp.describe().transpose()

df_exp.shape

movement = np.where(df_exp.Movement != 'no_movement', 1, 0)
plt.plot(movement)
plt.show()

"""# Splitting the Data"""

def separate_ids(ids):
    np.random.seed(42)
    ids_unique = list(set(ids))
    valid_test = np.random.choice(ids_unique, size=20, replace=False)
    train = [id for id in ids_unique if id not in valid_test]
    test = np.random.choice(valid_test, size=10, replace=False)
    valid = [v for v in valid_test if v not in test]
    return train, valid, test

import pandas as pd

def get_dfs_lists():
    file_dict = get_files_dict(base_path)

    data = []
    ids = []

    train_data = []
    valid_data = []
    test_data = []

    train_features = []
    valid_features = []
    test_features = []

    train_labels = []
    valid_labels = []
    test_labels = []

    for experiment, files in file_dict.items():
        if len(files) == 1:
            continue

        # Initialize these to None
        df_1, df_2, meta_data = None, None, None

        # Reading the first file for metadata and sensor data
        try:
            meta_data, df_1 = read_pbz2(files[0])
            animal_name = meta_data['animal']['animalName']
        except Exception as e:
            print(f"Skipping file due to error reading pbz2 file: {files[0]}, Error: {e}")
            continue

        # Reading the second file for CSV data
        try:
            df_2 = pd.read_csv(files[1])
        except Exception as e:
            print(f"Skipping file due to error reading CSV: {files[1]}, Error: {e}")
            continue

        # Merging and resetting index
        df_exp = merge_dfs(df_1, df_2)
        df_exp.reset_index(drop=True, inplace=True)

        # Deleting specific columns
        columns_to_drop = ['grav_x', 'grav_y', 'grav_z', 'att_w', 'att_x', 'att_y', 'att_z']
        df_exp.drop(columns=columns_to_drop, inplace=True, errors='ignore')

        if not df_exp.empty:
            data.append(df_exp)
            ids.append(animal_name)

    # Separate IDs for training, validation, and test
    train_id, valid_id, test_id = separate_ids(ids)

    # Allocating data to training, validation, or test based on the IDs
    for idx, df in enumerate(data):
        if ids[idx] in train_id:
            train_data.append(df)
        elif ids[idx] in valid_id:
            valid_data.append(df)
        elif ids[idx] in test_id:
            test_data.append(df)

    # Calculate global mean and std from training data
    all_train_data = pd.concat(train_data)
    train_mean = all_train_data.mean(numeric_only=True)
    train_std = all_train_data.std(numeric_only=True)

    # Normalization and feature-label separation
    for dataset, feature_list, label_list in zip((train_data, valid_data, test_data),
                                                 (train_features, valid_features, test_features),
                                                 (train_labels, valid_labels, test_labels)):
        for df in dataset:
            y = df['Movement'].values
            x = (df.drop('Movement', axis=1) - train_mean) / train_std
            feature_list.append(x)
            label_list.append(y)

    return (train_features, valid_features, test_features, train_labels, valid_labels, test_labels)

train_features, valid_features, test_features, train_labels, valid_labels, test_labels = get_dfs_lists()

df_std = train_features[0]
df_std = df_std.melt(var_name='Column', value_name='Normalized')
plt.figure(figsize=(12, 6))
ax = sns.violinplot(x='Column', y='Normalized', data=df_std)
plt.xticks(rotation=45)  # Rotates the labels on the x-axis for better readability
plt.title('Distribution of Normalized Features')
plt.show()

"""# Data Windowing"""

# construct a function to generate windowed data
def generate_window(x, y, window_length, stride):
    """
    Generate windows of X, and y arrays for time series classification

    Parameters:
    x (2d numpy array): Two dimensional numpy array with axis 0 as the readings
                        and axis 1 as the variables
    y (1d numpy array): One dimensional numpy array with labels
    window_length (int): Integer specifying the length of the instances
    stride (int): Integer specifying the overlap (step jump)

    Returns:
    tuple (3d numpy array, 1d numpy array): A tuple of numpy arrays; the first
            with shape (instances, window_length, variables) and
            the second specifying the labels using the most frequent value over the time steps
    """
    # Generate windows for the input features
    x = sliding_window_view(x, (window_length, x.shape[1]))[::stride, ...]

    # Generate windows for the labels
    y = sliding_window_view(y, window_length)[::stride]

    # Compute the mode for each window
    y_mode = np.array([np.bincount(window).argmax() for window in y])

    return x, y_mode

def get_data(sub):
    # Retrieve the data lists for the specific subset
    features, labels = get_dfs_lists(sub)

    # New lists to hold the windowed data
    X_list = []
    Y_list = []

    # Specify the window length and stride
    window_length = 100
    stride = 50

    # Process each pair of features and labels
    for x, y in zip(features, labels):
        x_w, y_w = generate_window(x, y, window_length, stride)
        X_list.append(x_w)
        Y_list.append(y_w)

    # Concatenate all data into numpy arrays
    X = np.concatenate(X_list, axis=0)
    Y = np.concatenate(Y_list, axis=0)

    return X, Y

X_train, y_train = get_data('train')

X_valid, y_valid = get_data('valid')

X_test, y_test = get_data('test')

X_train.shape, y_train.shape

X_valid.shape, y_valid.shape

X_test.shape, y_valid.shape